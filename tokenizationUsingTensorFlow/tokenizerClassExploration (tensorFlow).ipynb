{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN NAMES\n",
      "------------\n",
      "id\n",
      "dateAdded\n",
      "dateUpdated\n",
      "name\n",
      "brand\n",
      "categories\n",
      "primaryCategories\n",
      "manufacturer\n",
      "manufacturerNumber\n",
      "reviews.date\n",
      "reviews.doRecommend\n",
      "reviews.numHelpful\n",
      "reviews.rating\n",
      "reviews.text\n",
      "reviews.title\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# The whole data set\n",
    "\n",
    "# Generalized code for accessing the data directory\n",
    "# (Meant to work even if this file is within some other subdirectory)\n",
    "path = \"data/amazonConsumerReviews.csv\"\n",
    "while True:\n",
    "    try:\n",
    "        data = pd.read_csv(path)\n",
    "        break\n",
    "    except:\n",
    "        path = \"../\" + path\n",
    "print(\"COLUMN NAMES\\n------------\")\n",
    "for c in data.columns: print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only keeping relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "      <td>Too small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>Great light reader. Easy to use at the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>Great for the price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  reviews.doRecommend  reviews.rating  \\\n",
       "0  AVqVGZNvQMlgsOJE6eUY                False               3   \n",
       "1  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "2  AVqVGZNvQMlgsOJE6eUY                 True               4   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I thought it would be as big as small paper bu...   \n",
       "1  This kindle is light and easy to use especiall...   \n",
       "2  Didnt know how much i'd use a kindle so went f...   \n",
       "\n",
       "                                  reviews.title  \n",
       "0                                     Too small  \n",
       "1  Great light reader. Easy to use at the beach  \n",
       "2                           Great for the price  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only selecting relevant columns\n",
    "reviewsData = data[['id',\n",
    "                  'reviews.doRecommend',\n",
    "                  'reviews.rating',\n",
    "                  'reviews.text',\n",
    "                  'reviews.title']]\n",
    "reviewsData.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the words within the reviews\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "reviews = reviewsData['reviews.text'].values\n",
    "tokenizer = Tokenizer(num_words = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Tokenizer' class in the 'tensorflow.keras.preprocessing.text' module enables you to tokenize text. Tokenizing text is the process of breaking a text into tokens (usually individual words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_keras_api_names\n",
      "_keras_api_names_v1\n",
      "char_level\n",
      "document_count\n",
      "filters\n",
      "fit_on_sequences\n",
      "fit_on_texts\n",
      "get_config\n",
      "index_docs\n",
      "index_word\n",
      "lower\n",
      "num_words\n",
      "oov_token\n",
      "sequences_to_matrix\n",
      "sequences_to_texts\n",
      "sequences_to_texts_generator\n",
      "split\n",
      "texts_to_matrix\n",
      "texts_to_sequences\n",
      "texts_to_sequences_generator\n",
      "to_json\n",
      "word_counts\n",
      "word_docs\n",
      "word_index\n"
     ]
    }
   ],
   "source": [
    "# Using dir(tokenizer) to see what attributes this object has\n",
    "for a in dir(tokenizer): print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing texts\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informative attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer.word_index:\n",
      "('i', 1)\n",
      "('it', 2)\n",
      "('a', 3)\n",
      "('the', 4)\n",
      "('my', 5)\n",
      "('to', 6)\n",
      "\n",
      "tokenizer.word_counts:\n",
      "('i', 12)\n",
      "('thought', 1)\n",
      "('it', 10)\n",
      "('would', 2)\n",
      "('be', 3)\n",
      "('as', 3)\n",
      "\n",
      "tokenizer.word_docs:\n",
      "('read', 2)\n",
      "('recommend', 1)\n",
      "('very', 1)\n",
      "('would', 1)\n",
      "('big', 1)\n",
      "('on', 2)\n"
     ]
    }
   ],
   "source": [
    "# Extra function to print the first few elements of a collection\n",
    "def printn(collection, n):\n",
    "    for i, t in enumerate(collection):\n",
    "        if i > n: break\n",
    "        else: print(t)\n",
    "\n",
    "print(\"\\ntokenizer.word_index:\")\n",
    "printn(tokenizer.word_index.items(), 5)\n",
    "print(\"\\ntokenizer.word_counts:\")\n",
    "printn(tokenizer.word_counts.items(), 5)\n",
    "print(\"\\ntokenizer.word_docs:\")\n",
    "printn(tokenizer.word_docs.items(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see that tokenization using the 'Tokenizer' class associates each word with an index, word count and the number of documents i.e. separate texts in which each word occurs. This data is stored in the attributes word_index, word_count and word_docs respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
